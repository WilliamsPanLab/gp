---
title: "SampleConstruction"
author: "Adam"
output: github_document
date: "2023-01-25"
---

This .rmd links ndar downloads into a master dataset. Cross-sectional and temporal precedence datasets are be exported from this file (no matched imaging groups needed, pooled factor decomposition).

```{r}
#### load libraries
library(rapportools)
library(reshape2)
library(ggplot2)
library(ggalluvial)

###########∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆##############
# chunk 1 processes mental health data (CBCL)  #
###########∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆##############

### LOAD in cbcl data
cbcl=read.delim('~/Downloads/Package_1210940/abcd_cbcl01.txt')
cbcls=read.delim('~/Downloads/Package_1210940/abcd_cbcls01.txt')
# subset timepoints
cbclsBV=subset(cbcls,eventname=='baseline_year_1_arm_1')
cbcls2=subset(cbcls,eventname=='2_year_follow_up_y_arm_1')
# subset timepoints
cbclBV=subset(cbcl,eventname=='baseline_year_1_arm_1')
cbcl2=subset(cbcl,eventname=='2_year_follow_up_y_arm_1')
# merge with other cbcl
cbclsBV=merge(cbclsBV,cbclBV,by=c('subjectkey','eventname'))
cbcls2=merge(cbcls2,cbcl2,by=c('subjectkey','eventname'))

# initialize master df
masterdf<-merge(cbcls,cbcl,by=c('subjectkey','eventname','interview_age','src_subject_id','sex'))
# omit nans and empties for variables of interest (totprobs,int,ext)
masterdf=masterdf[!is.empty(masterdf$cbcl_scr_syn_totprob_r),]
masterdf=masterdf[!is.na(masterdf$cbcl_scr_syn_totprob_r),]
masterdf=masterdf[!is.empty(masterdf$cbcl_scr_syn_internal_r),]
masterdf=masterdf[!is.na(masterdf$cbcl_scr_syn_internal_r),]
masterdf=masterdf[!is.empty(masterdf$cbcl_scr_syn_external_r),]
masterdf=masterdf[!is.na(masterdf$cbcl_scr_syn_external_r),]

# initialize included subjects df
includedSubjects=data.frame(unique(masterdf$subjectkey))
colnames(includedSubjects)<-'subj'
includedSubjects$cbclInclude=1

# check for completeness at both timepoints- subset those timepoints
masterdf=masterdf[masterdf$eventname!='1_year_follow_up_y_arm_1',]
masterdf=masterdf[masterdf$eventname!='3_year_follow_up_y_arm_1',]
# calculate remaining subjs
cbclSubjs=length(unique(masterdf$subjectkey))
length(unique(masterdf$subjectkey))

# get other vars of interest to check for complete cases
KidVarsOfInt=c('cbcl_scr_syn_totprob_r','cbcl_scr_syn_external_r','cbcl_scr_syn_internal_r')

# only use subjects with both timepoints as complete cases
subjs=unique(masterdf$subjectkey)
for (s in subjs){
  # if there are less than two complete cases of the variables of interest
  if (sum(complete.cases(masterdf[masterdf$subjectkey==s,c(KidVarsOfInt)]))<2){
    subjs=subjs[subjs!=s]
  }
}

# exclude participants without data at both timepoints
cbclSubjs=length(unique(masterdf$subjectkey))
masterdf=masterdf[masterdf$subjectkey %in% subjs,]

# included subjs df
includedSubjects$CBCLBoth=0
includedSubjects[includedSubjects$subj %in% unique(masterdf$subjectkey),]$CBCLBoth=1

cbclSubjsBoth=length(unique(masterdf$subjectkey))
print(paste0(cbclSubjs-cbclSubjsBoth,' lost due to single-timepoint CBCL completeness'))
```

```{r}
###########∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆##############
####  Chunk 2   processes family ID   data  ####
###########∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆##############

# load in acs file for rel_family_ID
# get family ID from acs
acs=read.delim('~/Downloads/Package_1210940/acspsw03.txt')
acs$subjectkey<-acs$src_subject_id
acs=acs[,c('rel_family_id','subjectkey','eventname')]
# looks like all family IDs are missing from timepoint 1, so exclude for ease
acs=acs[acs$eventname=='baseline_year_1_arm_1',]
# and isolate family ID for ease
acs=data.frame(acs$subjectkey,acs$rel_family_id)
colnames(acs)=c('subjectkey','rel_family_id')
masterdf<-merge(masterdf,acs,by=c('subjectkey'))
# na omitted version
masterdf=masterdf[!is.na(masterdf$rel_family_id),] 
masterdf=masterdf[!is.empty(masterdf$rel_family_id),] 
acsSubjs=length(unique(masterdf$subjectkey))
# add to included subjs DF
includedSubjects$acs=0
includedSubjects[includedSubjects$subj %in% unique(masterdf$subjectkey),]$acs=1
# print data volume
print(acsSubjs)
# evaluate subjects lost
dif=cbclSubjsBoth-acsSubjs
###################

print(paste0(dif,' participants lost from ACS merge for family ID'))
```

```{r}
###########∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆##############
### This  (short)  chunk formats   cbcl data ###
###########∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆##############

### Clean data
# subjectkey as factor
masterdf$subjectkey<-as.factor(masterdf$subjectkey)
# convert cbcl scores to numeric
masterdf$cbcl_scr_syn_totprob_r<-as.numeric(masterdf$cbcl_scr_syn_totprob_r)
masterdf$cbcl_scr_syn_internal_r<-as.numeric(masterdf$cbcl_scr_syn_internal_r)
masterdf$cbcl_scr_syn_external_r<-as.numeric(masterdf$cbcl_scr_syn_external_r)

```

```{r}
###########∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆##############
####   Chunk 4 processes cognitive data    ####
###########∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆##############

#### LOAD in cognitive data
nihCog=read.delim('~/Downloads/Package_1210940/abcd_tbss01.txt')
othCog=read.delim('~/Downloads/Package_1210940/abcd_ps01.txt')
littleMan=read.delim('~/Downloads/Package_1210940/lmtp201.txt')

# merge in
masterdf<-merge(masterdf,nihCog,by=c('subjectkey','eventname','interview_age','sex'))
newList=length(unique(masterdf$subjectkey))
print(paste0(newList,' after merging nih toolbox, ',(acsSubjs- newList),' lost after merge'))
masterdf<-merge(masterdf,othCog,by=c('subjectkey','eventname','interview_age','sex'))
newList2=length(unique(masterdf$subjectkey))
print(paste0(newList2,' after merging other cognitive measures, ',(newList- newList2),' lost after merge'))
masterdf<-merge(masterdf,littleMan,by=c('subjectkey','eventname','interview_age','sex'))
newList3=length(unique(masterdf$subjectkey))
print(paste0(newList3,' after merging little man, ',(newList2 - newList3),' lost after merge'))

# clean age
masterdf$interview_age<-as.numeric(masterdf$interview_age)
masterdf$interview_age<-as.numeric(masterdf$interview_age)/12
```

```{r}
###########∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆##############
## Chunk 5 preps for cognition factorization ###
###########∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆##############

# use thompson 2019 recreation of non nih-tb measures
ind_pea_ravlt = c(which(names(masterdf)=="pea_ravlt_sd_trial_i_tc"),which(names(masterdf)=="pea_ravlt_sd_trial_ii_tc"),
	which(names(masterdf)=="pea_ravlt_sd_trial_iii_tc"),which(names(masterdf)=="pea_ravlt_sd_trial_iv_tc"),
	which(names(masterdf)=="pea_ravlt_sd_trial_v_tc")); names(masterdf)[ind_pea_ravlt];

# set numbers to numeric
masterdf$pea_ravlt_sd_trial_i_tc=as.numeric(masterdf$pea_ravlt_sd_trial_i_tc)
masterdf$pea_ravlt_sd_trial_ii_tc=as.numeric(masterdf$pea_ravlt_sd_trial_ii_tc)
masterdf$pea_ravlt_sd_trial_iii_tc=as.numeric(masterdf$pea_ravlt_sd_trial_iii_tc)
masterdf$pea_ravlt_sd_trial_iv_tc=as.numeric(masterdf$pea_ravlt_sd_trial_vi_tc)
masterdf$pea_ravlt_sd_trial_v_tc=as.numeric(masterdf$pea_ravlt_sd_trial_v_tc)

# total correct across trials
masterdf$pea_ravlt_ld = masterdf$pea_ravlt_sd_trial_i_tc + masterdf$pea_ravlt_sd_trial_ii_tc + masterdf$pea_ravlt_sd_trial_iii_tc + masterdf$pea_ravlt_sd_trial_iv_tc + masterdf$pea_ravlt_sd_trial_v_tc

# change to numeric
masterdf$nihtbx_picvocab_uncorrected<-as.numeric(masterdf$nihtbx_picvocab_uncorrected)
masterdf$nihtbx_flanker_uncorrected<-as.numeric(masterdf$nihtbx_flanker_uncorrected)
masterdf$nihtbx_list_uncorrected<-as.numeric(masterdf$nihtbx_list_uncorrected)
masterdf$nihtbx_cardsort_uncorrected<-as.numeric(masterdf$nihtbx_cardsort_uncorrected)
masterdf$nihtbx_pattern_uncorrected<-as.numeric(masterdf$nihtbx_pattern_uncorrected)
masterdf$nihtbx_picture_uncorrected<-as.numeric(masterdf$nihtbx_picture_uncorrected)
masterdf$nihtbx_reading_uncorrected<-as.numeric(masterdf$nihtbx_reading_uncorrected)
masterdf$pea_wiscv_tss<-as.numeric(masterdf$pea_wiscv_tss)
masterdf$lmt_scr_perc_correct<-as.numeric(masterdf$lmt_scr_perc_correct)

# for isolating PCA dataframe
pcVars=c("nihtbx_picvocab_uncorrected","nihtbx_flanker_uncorrected","nihtbx_pattern_uncorrected","nihtbx_picture_uncorrected","nihtbx_reading_uncorrected","pea_ravlt_ld","lmt_scr_perc_correct")

# only use subjects with both timepoints as complete cases
subjs=unique(masterdf$subjectkey)
for (s in subjs){
  # if there are less than two complete cases of the variables of interest
  if (sum(complete.cases(masterdf[masterdf$subjectkey==s,c(pcVars)]))<2){
    subjs=subjs[subjs!=s]
  }
}
# convert masterdf to df with complete observations for cognition
masterdf=masterdf[masterdf$subjectkey %in% subjs,]
newList4=length(unique(masterdf$subjectkey))
print(paste0(newList4,' after retaining only subjs with Cognitive vars of interest at BOTH timepoints, ',(newList3- newList4),' lost after removing'))

# populated included subjs df
includedSubjects$CogBoth=0
includedSubjects[includedSubjects$subj %in% unique(masterdf$subjectkey),]$CogBoth=1
```

```{r}
###########∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆##############
###  Chunk 6 preps by selecting 1 per family ###
###########∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆##############
# finish cleaning data for sherlock runs: one family member per family to facilitate random sample
masterdf$id_fam = NULL
# default value of family size (# of children in abcd study)
masterdf$fam_size = 1

# counter index
ind=0

# set each instance of multiple family members to a family ID, as ind
set.seed(1)
for(f in 1:length(unique(masterdf$rel_family_id))){
  # calculate family size
  famsize=sum(masterdf$rel_family_id == unique(masterdf$rel_family_id)[f]) / 2
  masterdf$fam_size[masterdf$rel_family_id == unique(masterdf$rel_family_id)[f]] = famsize
  # note that each  person is represented twice at this point:
  # divide by 2 to take number of visits to number of people, if there's more than 2x visits per family ID, izza family
  # this logic gets hairy. Starting from outside in: > 1 is family size >1, /2 is divided by 2 for two visits, [f] is unique familyID, rel_family_id is place in column of masterdf
  if(famsize>1){
    # remove one from instances where family-id = this relative family id (sequence for siblings, 1:size(Family))
    #print(paste0('family size ',famsize))
    # keep one sib
    kept=sample(seq(1,famsize),1)
    #print(paste0('kept ',kept))
    # use to select one subject id
    famIDs=unique(masterdf$subjectkey[masterdf$rel_family_id == unique(masterdf$rel_family_id)[f]])
    # chosen sib
    keeper=famIDs[kept]
    left=famIDs[-c(kept)]
    # leave rest
    masterdf=masterdf[!masterdf$subjectkey %in% left,] 
    # calc index of family
    ind=ind+1	
    # set index of family
    masterdf$id_fam[masterdf$rel_family_id == unique(masterdf$rel_family_id)[f]] = ind
  }	
}

# make family ID for those with families represented in ABCD
masterdf$rel_family_id=masterdf$id_fam

newList5=length(unique(masterdf$subjectkey))
print(paste0(newList5,' after retaining only one subjs per family, ',(newList4- newList5),' lost after removing'))

# included subjects DF to track subj loss
includedSubjects$OnePerFamily=0
includedSubjects[includedSubjects$subj %in% unique(masterdf$subjectkey),]$OnePerFamily=1

# pea_wiscv_tss, nihtbx_list_uncorrected, and nihtbx_cardsort_uncorrected taken out for lack of longitudinal coverage
pcaDf<-masterdf[,pcVars]
```

```{r}
###########∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆##############
### Chunk 7: it runs cognition factorization ###
###########∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆##############

# derive pcs
Y = as.matrix(scale(pcaDf[complete.cases(pcaDf[,pcVars]),pcVars]))
# equiv for binding scores to IDs and eventnames
pcVarsAndIDs=c("nihtbx_picvocab_uncorrected","nihtbx_flanker_uncorrected","nihtbx_pattern_uncorrected","nihtbx_picture_uncorrected","nihtbx_reading_uncorrected","pea_ravlt_ld","lmt_scr_perc_correct","subjectkey","eventname")
Yextended=masterdf[complete.cases(masterdf[,pcVarsAndIDs]),pcVarsAndIDs]
ncomp = 3
y.pca = psych::principal(Y, rotate="varimax", nfactors=ncomp, scores=TRUE)
y.pca$loadings
# assign scores to subjs
Yextended$g<-y.pca$scores[,1]

# merge in cog data
masterdf$g<-Yextended$g
```

```{r}
###########∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆##############
## Chunk 8 subjects with data at both timepoints##
###########∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆##############

# exclude subjs without data for both timepoints
OutDFsubjects=length(unique(masterdf$subjectkey))
OutDFBV=subset(masterdf,eventname=='baseline_year_1_arm_1')
OutDF2Y=subset(masterdf,eventname=='2_year_follow_up_y_arm_1')
# intersection of subjs in both
BothTPsubjs=intersect(OutDFBV$subjectkey,OutDF2Y$subjectkey)
Different=setdiff(unique(masterdf$subjectkey),BothTPsubjs)
# print sanity check subjs lost to another check of both tps populated
newList6=length(unique(BothTPsubjs))
dif=newList5-newList6
print(paste0(dif,' rows lost from only using subjs with both timepoints'))

```

```{r}
###########∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆##############
## Chunk 9 Handles participants TSV          ##
###########∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆##############

### LOAD in ParticipantsTSV for parent income and edu background
# ordained sample split
participantsTSV=read.delim('~/Downloads/participants.tsv',sep="\t")
participantsTSV$subjectkey<-participantsTSV$participant_id
# reformat participant IDs so they match everything else
participantsTSV$subjectkey<-gsub('sub-','',participantsTSV$subjectkey)
participantsTSV$subjectkey<-as.factor(gsub('NDARINV','NDAR_INV',participantsTSV$subjectkey))
participantsTSV$eventname=participantsTSV$session_id
### issue where subjects are repeated. Rows are not identical either.
# query. get table of subject ids
b=table(as.factor(participantsTSV$participant_id))
# report subj ids used more than once and times used
bdf=data.frame(b[b>1])
SubjsRepeated=bdf$Var1
# well some rows with the same participant IDs have different sites, can't use that unless it makes sense
dimB4repRem=dim(participantsTSV)
# remove repeated subjs
participantsTSV=participantsTSV[!participantsTSV$participant_id %in% SubjsRepeated,]
dif=dimB4repRem[1]-dim(participantsTSV)[1]
print(paste0(dif/2,' Participants lost from ambiguously repeated pt ids in participants.tsv'))
# well some rows with the same participant IDs have different sites, can't use that unless it makes sense\
dimB4repRem=dim(participantsTSV)
# remove repeated subjs
participantsTSV=participantsTSV[!participantsTSV$participant_id %in% SubjsRepeated,]
# remove eventname column from participants tsv, not informative and causes problems down the road
participantsTSV = participantsTSV[,!(names(participantsTSV) %in% 'eventname')]
# convert sex to M/F in particpants tsv
participantsTSV$sex[participantsTSV$sex==1]="M"
participantsTSV$sex[participantsTSV$sex==2]="F"

# take out na incomes
participantsTSV=participantsTSV[participantsTSV$income!=777,]
participantsTSV=participantsTSV[participantsTSV$income!=999,]
# race
participantsTSV=participantsTSV[participantsTSV$race_ethnicity!=888,]
participantsTSV$race_ethnicity<-as.factor(participantsTSV$race_ethnicity)
# parental edu
participantsTSV=participantsTSV[participantsTSV$parental_education!=888,]
participantsTSV=participantsTSV[participantsTSV$parental_education!=777,]
participantsTSV$parental_education<-as.ordered(participantsTSV$parental_education)


### merge in for fam income and parent edu
masterdf=merge(masterdf,participantsTSV,by=c('subjectkey','sex'))
participantsTSVSubjs=length(unique(masterdf$subjectkey))
# add to included subjs DF
includedSubjects$pTSV=0
includedSubjects[includedSubjects$subj %in% unique(masterdf$subjectkey),]$pTSV=1



#### MIGHTG NEED ADAPTING
print(paste0(newList6-participantsTSVSubjs,' participants lost after needing complete data in participantstsv'))
paste0(participantsTSVSubjs,' remain')
```

```{r,warning=FALSE}

###########∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆##############
## Chunk 10: higher-resolution demographics     ##
###########∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆##############

#### get more fine-grained demographics
fg_demo=read.delim('~/Downloads/Package_1223147/abcd_lpds01.txt',sep="\t")
# INR is Household Income : Poverty line for this # of people
# convert Household income
HouseholdIncome=as.factor(fg_demo$demo_comb_income_v2_l)
# to be replaced with numerican "median" income
fg_demo$HouseholdIncome=NULL
# to be replace with number of household members
fg_demo$NumPeeps=NULL
# to be replaced with number of people-specific poverty line
fg_demo$PL=NULL
# to be replaced with INR
fg_demo$INR=NULL

# 1 is less than 5k
fg_demo$HouseholdIncome[fg_demo$demo_comb_income_v2_l==1]=median(c(0,5000))
# 2 is 5-12k
fg_demo$HouseholdIncome[fg_demo$demo_comb_income_v2_l==2]=median(c(5000,11999))
# 3 is 12-16k
fg_demo$HouseholdIncome[fg_demo$demo_comb_income_v2_l==3]=median(c(12000,15999))
# 4 is 16-25k
fg_demo$HouseholdIncome[fg_demo$demo_comb_income_v2_l==4]=median(c(16000,24999))
# 5 25-35 k
fg_demo$HouseholdIncome[fg_demo$demo_comb_income_v2_l==5]=median(c(25000,34999))
# 6 35-50 k
fg_demo$HouseholdIncome[fg_demo$demo_comb_income_v2_l==6]=median(c(35000,49999))
# 7 50-75 k
fg_demo$HouseholdIncome[fg_demo$demo_comb_income_v2_l==7]=median(c(50000,74999))
# 8 75-100 k
fg_demo$HouseholdIncome[fg_demo$demo_comb_income_v2_l==8]=median(c(75000,99999))
# 9 100-200 k
fg_demo$HouseholdIncome[fg_demo$demo_comb_income_v2_l==9]=median(c(100000,199999))
# 10 > 200k: note 500k is top bound for median purposes
fg_demo$HouseholdIncome[fg_demo$demo_comb_income_v2_l==10]=median(c(200000,500000))
# 999 idk
fg_demo$HouseholdIncome[fg_demo$demo_comb_income_v2_l==999]=NA
# 777 refuse to answer
fg_demo$HouseholdIncome[fg_demo$demo_comb_income_v2_l==777]=NA
# convert number of people
fg_demo$NumPeeps=as.numeric(fg_demo$demo_roster_v2_l)
# Separate out income bins to match to their poverty line: https://aspe.hhs.gov/topics/poverty-economic-mobility/poverty-guidelines/prior-hhs-poverty-guidelines-federal-register-references/2017-poverty-guidelines
# 12,060 for 1 (note 1 should not be included in analyses, indicates child does not live there)
fg_demo$PL[fg_demo$NumPeeps==1]=12060
# 16,240 for 2
fg_demo$PL[fg_demo$NumPeeps==2]=16240
# 20,420 for 3
fg_demo$PL[fg_demo$NumPeeps==3]=20420
# 24,600 for 4
fg_demo$PL[fg_demo$NumPeeps==4]=24600
# 28,780 for 5
fg_demo$PL[fg_demo$NumPeeps==5]=28780
# 32,960 for 6
fg_demo$PL[fg_demo$NumPeeps==6]=32960
# 37,140 for 7
fg_demo$PL[fg_demo$NumPeeps==7]=37140
# 41,320 for 8
fg_demo$PL[fg_demo$NumPeeps==8]=41320
# 41,320 + 4,180 for 9
fg_demo$PL[fg_demo$NumPeeps==9]=45300
# 41,320 + 2*(4,180) for 10
fg_demo$PL[fg_demo$NumPeeps==10]=49480
# 41,320 + 3*(4,180) for 11
fg_demo$PL[fg_demo$NumPeeps==11]=53660
# 41,320 + 4*(4,180) for 12
fg_demo$PL[fg_demo$NumPeeps==12]=57840
# 41,320 + 5*(4,180) for 13
fg_demo$PL[fg_demo$NumPeeps==13]=62020
# 41,320 + 6*(4,180) for 14
fg_demo$PL[fg_demo$NumPeeps==14]=66200
# 41,320 + 7*(4,180) for 15
fg_demo$PL[fg_demo$NumPeeps==15]=70380
# 41,320 + 8*(4,180) for 16
fg_demo$PL[fg_demo$NumPeeps==16]=74560

# drop interview age for merge
fg_demo <- fg_demo[, !(names(fg_demo) %in% "interview_age")]
# drop eventname for merge
fg_demo <- fg_demo[, !(names(fg_demo) %in% "eventname")]


# INR is Household Income : Poverty line for this # of people
fg_demo$INR=fg_demo$HouseholdIncome/fg_demo$PL
# new pov classification
fg_demo$Pov_v2=as.numeric(fg_demo$HouseholdIncome<fg_demo$PL)

# retain only those with actual INR data
fg_demo=fg_demo[!is.na(fg_demo$INR),]

# number of rows should be = number of unique subjects
duplicated_rows <- duplicated(fg_demo$subjectkey)
fg_demo_unique <- fg_demo[!(duplicated_rows & is.na(fg_demo$INR)), ]

# now populate both observations in masterdf
masterdf$Pov_v2 <- 0
masterdf$INR <- NA
masterdf$NumPeeps <-NA
masterdf$HouseholdIncome <- NA

# loop over each subject in masterdf
for (subject in unique(fg_demo_unique$subjectkey)) {
  subjrow=fg_demo_unique[fg_demo_unique$subjectkey == subject,]
  # insert
  masterdf$Pov_v2[masterdf$src_subject_id.x == subject] <- subjrow$Pov_v2
  masterdf$INR[masterdf$src_subject_id.x == subject] <- subjrow$INR
  masterdf$NumPeeps[masterdf$src_subject_id.x == subject] <- subjrow$NumPeeps
  masterdf$HouseholdIncome[masterdf$src_subject_id.x == subject] <- subjrow$HouseholdIncome
}

# omit missing values
masterdf=masterdf[!is.na(masterdf$INR),]

# omit subject with g < -7
kidWhoDidntTry=masterdf$subjectkey[masterdf$g < -7]
masterdf=masterdf[masterdf$subjectkey!=kidWhoDidntTry,]

# new merge and count
parentDemo_TSVSubjs=length(unique(masterdf$subjectkey))
print(paste0(participantsTSVSubjs-parentDemo_TSVSubjs,' participants lost after needing complete data in Parent demographics'))
paste0(parentDemo_TSVSubjs,' remain')
# add to included subjs DF
includedSubjects$PD=0
includedSubjects[includedSubjects$subj %in% unique(masterdf$subjectkey),]$PD=1

#########################
#########################
#########################
# this dataframe is now your working data frame for main figure RMDs
saveRDS(masterdf,'~/gp_masterdf.rds')
####################
#########################
#########################
#########################

# split halves to retain subjects entirely in one df or the other
unique_subjects <- unique(masterdf$subjectkey)

# to get same split on re-runs
set.seed(1)

shuffled_subjects <- sample(unique_subjects)

# Step 3: Split the unique subject keys into two groups
half_index <- length(shuffled_subjects) %/% 2
group1_subjects <- shuffled_subjects[1:half_index]
group2_subjects <- shuffled_subjects[(half_index + 1):length(shuffled_subjects)]

# Step 4: Split the original dataframe into two, based on the groups
df_group1 <- masterdf[masterdf$subjectkey %in% group1_subjects, ]
df_group2 <- masterdf[masterdf$subjectkey %in% group2_subjects, ]

saveRDS(df_group1,'~/gp_SH1_masterdf.rds')
saveRDS(df_group2,'~/gp_SH2_masterdf.rds')

```


```{r}
###################∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆##############
##  Chunk 11 saves out clinical thresholds for boots  ##
###################∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆##############
# pull clinical cutoff from master df: t scores > 65 = borderline clinical, 69 = clinical
masterdfP_bc<-masterdf[masterdf$cbcl_scr_syn_totprob_t==65,]
masterdfP_c<-masterdf[masterdf$cbcl_scr_syn_totprob_t==69,]
masterdfI_bc<-masterdf[masterdf$cbcl_scr_syn_internal_t==65,]
masterdfI_c<-masterdf[masterdf$cbcl_scr_syn_internal_t==69,]
masterdfE_bc<-masterdf[masterdf$cbcl_scr_syn_external_t==65,]
masterdfE_c<-masterdf[masterdf$cbcl_scr_syn_external_t==69,]
masterdfAnx_bc<-masterdf[masterdf$cbcl_scr_syn_anxdep_t==65,]
masterdfAnx_c<-masterdf[masterdf$cbcl_scr_syn_anxdep_t==69,]
# note no one has t==65 in this dataset for thought
masterdfTho_bc<-masterdf[masterdf$cbcl_scr_syn_thought_t==66,]
masterdfTho_c<-masterdf[masterdf$cbcl_scr_syn_thought_t==69,]
# note no one has t==65 in this dataset for withdrawn depression
masterdfWit_bc<-masterdf[masterdf$cbcl_scr_syn_withdep_t==66,]
masterdfWit_c<-masterdf[masterdf$cbcl_scr_syn_withdep_t==69,]
masterdfSom_bc<-masterdf[masterdf$cbcl_scr_syn_somatic_t==65,]
# no one has t==69
masterdfSom_c<-masterdf[masterdf$cbcl_scr_syn_somatic_t==70,]
masterdfSoc_bc<-masterdf[masterdf$cbcl_scr_syn_social_t==65,]
masterdfSoc_c<-masterdf[masterdf$cbcl_scr_syn_social_t==69,]
masterdfAtt_bc<-masterdf[masterdf$cbcl_scr_syn_attention_t==65,]
masterdfAtt_c<-masterdf[masterdf$cbcl_scr_syn_attention_t==69,]
masterdfRul_bc<-masterdf[masterdf$cbcl_scr_syn_rulebreak_t==65,]
masterdfRul_c<-masterdf[masterdf$cbcl_scr_syn_rulebreak_t==69,]
masterdfAgg_bc<-masterdf[masterdf$cbcl_scr_syn_aggressive_t==65,]
masterdfAgg_c<-masterdf[masterdf$cbcl_scr_syn_aggressive_t==69,]

# borderline clinical and clinical cutoffs
Pbc=mean(masterdfP_bc$cbcl_scr_syn_totprob_r)
Pc=mean(masterdfP_c$cbcl_scr_syn_totprob_r)
Ibc=mean(masterdfP_bc$cbcl_scr_syn_internal_r)
Ic=mean(masterdfP_c$cbcl_scr_syn_internal_r)
Ebc=mean(masterdfE_bc$cbcl_scr_syn_external_r)
Ec=mean(masterdfE_c$cbcl_scr_syn_external_r)
AnxBc=mean(as.numeric(masterdfAnx_bc$cbcl_scr_syn_anxdep_r))
AnxC=mean(as.numeric(masterdfAnx_c$cbcl_scr_syn_anxdep_r))
ThoBc=mean(as.numeric(masterdfTho_bc$cbcl_scr_syn_thought_r))
ThoC=mean(as.numeric(masterdfTho_c$cbcl_scr_syn_thought_r))
WitBc=mean(as.numeric(masterdfWit_bc$cbcl_scr_syn_withdep_r))
WitC=mean(as.numeric(masterdfWit_c$cbcl_scr_syn_withdep_r))
SomBc=mean(as.numeric(masterdfSom_bc$cbcl_scr_syn_somatic_r))
SomC=mean(as.numeric(masterdfSom_c$cbcl_scr_syn_somatic_r))
SocBc=mean(as.numeric(masterdfSom_bc$cbcl_scr_syn_social_r))
SocC=mean(as.numeric(masterdfSoc_c$cbcl_scr_syn_social_r))
AttBc=mean(as.numeric(masterdfAtt_bc$cbcl_scr_syn_attention_r))
AttC=mean(as.numeric(masterdfAtt_c$cbcl_scr_syn_attention_r))
RulBc=mean(as.numeric(masterdfRul_bc$cbcl_scr_syn_rulebreak_r))
RulC=mean(as.numeric(masterdfRul_c$cbcl_scr_syn_rulebreak_r))
AggBc=mean(as.numeric(masterdfAgg_bc$cbcl_scr_syn_aggressive_r))
AggC=mean(as.numeric(masterdfAgg_c$cbcl_scr_syn_aggressive_r))

# save out cuttoffs in RDS for sherlock bootstrapping
saveRDS(data.frame(Pbc,Pc,Ibc,Ic,Ebc,Ec,AnxBc,AnxC,ThoBc,ThoC,WitBc,WitC,SomBc,SomC,SocBc,SocC,AttBc,AttC,RulBc,RulC,AggBc,AggC),'~/gp_ClinCutoffs.rds')

```

```{r}
# with puberty data
###########∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆##############
###   Chunk 12    processes     puberty    data   ###
###########∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆##############
pub=read.delim('~/Downloads/Package_1226771/abcd_ssphp01.txt')
pub=pub[pub$eventname!='1_year_follow_up_y_arm_1',]
pub=pub[pub$eventname!='3_year_follow_up_y_arm_1',]
pub=pub[pub$eventname!='6_month_follow_up_arm_1',]
pub=pub[pub$eventname!='30_month_follow_up_arm_1',]
pub=pub[pub$eventname!='18_month_follow_up_arm_1',]
pub=pub[pub$eventname!='42_month_follow_up_arm_1',]

pub$interview_age<-as.numeric(pub$interview_age)
pub$interview_age<-as.numeric(pub$interview_age)/12

# merge in 
masterdf_pub<-merge(masterdf,pub,by=c('subjectkey','eventname','interview_age','sex'))

# looks like pds_p_ss_male_category and pds_p_ss_female_category are the fields of interest, but underpopulated
masterdf_pub_filtered <- masterdf_pub[!(masterdf_pub$pds_p_ss_male_category == "" & masterdf_pub$pds_p_ss_female_category == ""), ]

# make a unified category
# Use male category if available, otherwise use female category
masterdf_pub_filtered$stage <- ifelse(masterdf_pub_filtered$pds_p_ss_male_category != "", 
                                      masterdf_pub_filtered$pds_p_ss_male_category, 
                                      masterdf_pub_filtered$pds_p_ss_female_category)


# It looks like we lose around half of all observations if we need this measure. 
# Note that 5046/5812 of the remaining measures are baseline year 1 arm 1
# Most sensible choice is to drop the puberty measure at visit 2, as visit 2 is heavily underrepresented and puberty at timepoint 2 might have a different relationship to variables of interest than at timepoint 1
# proceed with single-observation per-subject modeling, i.e., no bootstraps
masterdf_pub=masterdf_pub_filtered[masterdf_pub_filtered$eventname=='baseline_year_1_arm_1',]

# evaluate impact of including puberty measure on g ~ p + age
fullMod=gam(g~s(cbcl_scr_syn_totprob_r,k=4)+s(interview_age,k=4)+stage,data=masterdf_pub)
reducedMod=gam(g~s(cbcl_scr_syn_totprob_r,k=4)+s(interview_age,k=4),data=masterdf_pub)

# evaluate impact of including puberty measure on g ~ p + age + income
fullMod=gam(g~s(cbcl_scr_syn_totprob_r,k=4)+s(interview_age,k=4)+stage+s(income,k=4),data=masterdf_pub)
reducedMod=gam(g~s(cbcl_scr_syn_totprob_r,k=4)+s(interview_age,k=4)+s(income,k=4),data=masterdf_pub)

fullMod=gam(g~s(cbcl_scr_syn_internal_r,k=4)+s(interview_age,k=4)+stage+s(income,k=4),data=masterdf_pub)
reducedMod=gam(g~s(cbcl_scr_syn_internal_r,k=4)+s(interview_age,k=4)+s(income,k=4),data=masterdf_pub)

fullMod=gam(g~s(cbcl_scr_syn_external_r,k=4)+s(interview_age,k=4)+stage+s(income,k=4),data=masterdf_pub)
reducedMod=gam(g~s(cbcl_scr_syn_external_r,k=4)+s(interview_age,k=4)+s(income,k=4),data=masterdf_pub)

# evaluate impact of including puberty measure on g ~ p + age + edu
#fullMod=gam(g~s(cbcl_scr_syn_totprob_r,k=4)+s(interview_age,k=4)+stage+s(parental_education,k=4),data=masterdf_pub)
#reducedMod=gam(g~s(cbcl_scr_syn_totprob_r,k=4)+s(interview_age,k=4)+s(parental_education,k=4),data=masterdf_pub)
# evaluate impact of including puberty measure on g ~ p + age + edu + income
#fullMod=gam(g~s(cbcl_scr_syn_totprob_r,k=4)+s(interview_age,k=4)+stage+s(parental_education,k=4)+s(income,k=4),data=masterdf_pub)
#reducedMod=gam(g~s(cbcl_scr_syn_totprob_r,k=4)+s(interview_age,k=4)+s(parental_education,k=4)+s(income,k=4),data=masterdf_pub)
```

```{r}
# with BPM
###########∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆##############
###   Chunk 13    processes     BPM    data   ###
###########∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆##############

### LOAD in BPM
bpm=read.delim('~/Downloads/Package_1226771/abcd_bpm01.txt')
bpm=bpm[bpm$eventname!='1_year_follow_up_y_arm_1',]
bpm=bpm[bpm$eventname!='3_year_follow_up_y_arm_1',]
bpm=bpm[bpm$eventname!='6_month_follow_up_arm_1',]
bpm=bpm[bpm$eventname!='30_month_follow_up_arm_1',]
bpm=bpm[bpm$eventname!='18_month_follow_up_arm_1',]
bpm=bpm[bpm$eventname!='42_month_follow_up_arm_1',]

# clean age
bpm$interview_age<-as.numeric(bpm$interview_age)
bpm$interview_age<-as.numeric(bpm$interview_age)/12

# Remove rows containing 777 or 999 in any column
bpm <- bpm[!apply(bpm, 1, function(row) any(row %in% c(777, 999))), ]


# recreate internalizing
bpm$bpInt=as.numeric(bpm$bpm_9_y)+as.numeric(bpm$bpm_11_y)+as.numeric(bpm$bpm_12_y)+as.numeric(bpm$bpm_13_y)+as.numeric(bpm$bpm_18_y)+as.numeric(bpm$bpm_19_y)
# recreate externalizing
bpm$bpExt=as.numeric(bpm$bpm_2_y)+as.numeric(bpm$bpm_6_y)+as.numeric(bpm$bpm_7_y)+as.numeric(bpm$bpm_8_y)+as.numeric(bpm$bpm_15_y)+as.numeric(bpm$bpm_16_y)+as.numeric(bpm$bpm_17_y)
# recreate attention
bpm$bpAtt=as.numeric(bpm$bpm_1_y)+as.numeric(bpm$bpm_3_y)+as.numeric(bpm$bpm_4_y)+as.numeric(bpm$bpm_5_y)+as.numeric(bpm$bpm_10_y)+as.numeric(bpm$bpm_14_y)
# recreate total problems
bpm$bpTot=as.numeric(bpm$bpm_1_y)+as.numeric(bpm$bpm_2_y)+as.numeric(bpm$bpm_3_y)+as.numeric(bpm$bpm_4_y)+as.numeric(bpm$bpm_5_y)+as.numeric(bpm$bpm_6_y)+as.numeric(bpm$bpm_7_y)+as.numeric(bpm$bpm_8_y)+as.numeric(bpm$bpm_9_y)+as.numeric(bpm$bpm_10_y)+as.numeric(bpm$bpm_11_y)+as.numeric(bpm$bpm_12_y)+as.numeric(bpm$bpm_13_y)+as.numeric(bpm$bpm_14_y)+as.numeric(bpm$bpm_15_y)+as.numeric(bpm$bpm_16_y)+as.numeric(bpm$bpm_17_y)+as.numeric(bpm$bpm_18_y)+as.numeric(bpm$bpm_19_y)

# merge and count losses
masterdf_BPM<-merge(masterdf,bpm,by=c('subjectkey','eventname','interview_age','sex'))

# add to included subjs DF
includedSubjects$BPM=0
includedSubjects[includedSubjects$subj %in% unique(masterdf_BPM$subjectkey),]$BPM=1
# print data volume
bpmSubjs=length(unique(masterdf_BPM$subjectkey))
print(bpmSubjs)
dif=cbclSubjsBoth-parentDemo_TSVSubjs
print(paste0(dif,' participants lost from BPM merge'))

# included subjs df
includedSubjects$BPM=0
includedSubjects[includedSubjects$subj %in% unique(masterdf_BPM$subjectkey),]$BPM=1
```

```{r}
# evaluate site effects
anova_result <- aov(g ~ site, data = masterdf)

# Calculate the mean of g for each site
mean_g_per_site <- aggregate(g ~ site, data = masterdf, FUN = mean, na.action = na.omit)

# Count the number of observations per site
n_per_site <- as.data.frame(table(masterdf$site))
names(n_per_site) <- c("site", "n")
site_stats <- merge(mean_g_per_site, n_per_site, by = "site")

ggplot(site_stats, aes(x = site, y = g, size = n)) +
  geom_point(alpha = 0.6) +  # Adjust transparency with alpha
  theme_minimal() +
  labs(x = "Site", y = "Average of g", size = "Number of Observations") +
  ggtitle("Average of g by Site with Observation Size")

# note the goal is to find which one is large and near the middle, so we can use it for predict() in slurm bootstraps to be fairly represntatitve without being crazy variable across bootstraps
```

```{r, echo=FALSE, fig.dim = c(25, 7)}
###########∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆##############
#### Chunk 15 plots data missingness      ######
###########∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆##############

# melt the plot df to get in proper format
plotdf=melt(includedSubjects)
plotdf$value<-as.factor(plotdf$value)
plotdf$subj<-as.factor(plotdf$subj)
# merge in raceEth of each subj
raceEth=participantsTSV$race_ethnicity
subjectsInPtsv=participantsTSV$subjectkey
infoDf=data.frame(raceEth,subjectsInPtsv)
colnames(infoDf)<-c('RaceEthn','subj')
plotdf2=merge(plotdf,infoDf,by='subj')

# remove participants tsv stratum, as the depicted stratum are predicated on participants tsv data being present
plotdf2=plotdf2[plotdf2$variable!='pTSV',]
# overwrite race with "empty" if missing after each checkpoint
plotdf2$RaceEthn<-factor(plotdf2$RaceEthn,levels=c(1,2,3,4,5,6),labels=c("White","Black","Hispanic","Asian","Other","Missing"))
plotdf2$RaceEthn[plotdf2$value==0]="Missing"
alph_order <- c("Asian", "Black", "Hispanic", "Missing", "Other", "White")

# preset some color
my_colors <- c("red", "orange", "blue", "gray", "green","yellow")

# preset x-axis names
x_labs=c("CBCL", "CBCL (2x)","Grades (2x)","ASR","ACS","Cognitive data (2x)","One sibling","Demographics")

#
ggplot(plotdf2, aes(x = variable, stratum = RaceEthn, alluvium = subj)) +
  geom_stratum(aes(fill = factor(RaceEthn, levels = alph_order))) +
  geom_flow(aes(fill = factor(RaceEthn, levels = alph_order))) +
  scale_fill_manual(values = my_colors, breaks = alph_order) +
  theme_bw(base_size = 30) + 
  labs(y = "Children", fill = "Race/Ethnicity", title = "Alluvial Consort Chart") +
  scale_x_discrete(labels = x_labs) +
  theme(axis.title.x = element_blank())
# saved at 2400x1000

```

```{r}
###########∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆##############
#### Chunk 16 plots  missingness as pie charts #
###########∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆##############

# can overlay two pie charts: starting raceEth Comp and Ending. Tuck into area that is pink now
startingdf=plotdf2[plotdf2$variable=='cbclInclude',]
startingdf=startingdf[startingdf$value==1,]
endingdf=plotdf2[plotdf2$variable=='PD',]
endingdf=endingdf[endingdf$value==1,]
# now tabulate them to get in proper plotting format
startingdfTab=tabulate(startingdf$RaceEthn)
endingdfTab=tabulate(endingdf$RaceEthn)
# starting df: adding 0 for placeholder for missing category (consistent coloration)
startingdf=data.frame(c(startingdfTab,0),factor(c("White","Black","Hispanic","Asian","Other","Missing"),levels=c("White","Black","Hispanic","Asian","Other","Missing")))
colnames(startingdf)<-c('value','RaceEthnicity')

# now make color assignment equivalent

ggplot(startingdf, aes(x="", y=value, fill = factor(RaceEthnicity, levels = alph_order))) +
  geom_bar(stat="identity", width=1) +
  coord_polar("y", start=0)+ggtitle('Before Exclusions')+
  theme(axis.text = element_blank(),
        axis.ticks = element_blank(),
        panel.grid  = element_blank(),panel.background = element_blank(),
        plot.title = element_text(size = 30),
        legend.text = element_text(size=30),legend.title = element_blank())+scale_fill_manual(values = my_colors)

# plot df is both with race labels
endingdf=data.frame(c(endingdfTab,0),factor(c("White","Black","Hispanic","Asian","Other","Missing"),levels=c("White","Black","Hispanic","Asian","Other","Missing")))
colnames(endingdf)<-c('value','RaceEthnicity')
ggplot(endingdf, aes(x="", y=value, fill = factor(RaceEthnicity, levels = alph_order))) +
  geom_bar(stat="identity", width=1) +
  coord_polar("y", start=0)+ggtitle('After Exclusions')+
  theme(axis.text = element_blank(),
        axis.ticks = element_blank(),
        panel.grid  = element_blank(),panel.background = element_blank(),
        plot.title = element_text(size = 30),
        legend.text = element_text(size=30),legend.title = element_blank())+scale_fill_manual(values = my_colors)
```





