---
title: "SampleConstruction"
author: "Adam"
output: github_document
date: "2023-01-25"
---

This .rmd links ndar downloads into a master dataset. Cross-sectional and temporal precedence datasets are be exported from this file (no matched imaging groups needed, pooled factor decomposition), while predictive datasets are exported from SampleConstruction_Ridge.Rmd

```{r}
#### LOAD libraries
library(rapportools)
```

```{r}
###########∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆##############
### This chunk processes mental health data ###
###########∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆##############

### LOAD in cbcl data
cbcl=read.delim('~/Downloads/Package_1205735/abcd_cbcl01.txt')
cbcls=read.delim('/Users/panlab/Downloads/Package_1205735/abcd_cbcls01.txt')
# subset timepoints
cbclsBV=subset(cbcls,eventname=='baseline_year_1_arm_1')
cbcls2=subset(cbcls,eventname=='2_year_follow_up_y_arm_1')
# subset timepoints
cbclBV=subset(cbcl,eventname=='baseline_year_1_arm_1')
cbcl2=subset(cbcl,eventname=='2_year_follow_up_y_arm_1')
# merge with other cbcl
cbclsBV=merge(cbclsBV,cbclBV,by=c('subjectkey','eventname'))
cbcls2=merge(cbcls2,cbcl2,by=c('subjectkey','eventname'))

# initialize master df
masterdf<-merge(cbcls,cbcl,by=c('subjectkey','eventname','interview_age','src_subject_id'))
cbcldim<-dim(masterdf)
print(cbcldim)


### LOAD in grades, ∆∆∆ will need to correct for incongruency between tp1 measure (decent granularity) and tp2 measure (high granularity) ∆∆∆
gradesInfoBV=readRDS('~/Downloads/DEAP-data-download-13.rds')
# extract baseline
gradesInfoBV=subset(gradesInfoBV,event_name=='baseline_year_1_arm_1')
gradesInfoBV$Grades<-as.numeric(gradesInfoBV$ksads_back_grades_in_school_p)
# convert ndar value to R
gradesInfoBV$Grades[gradesInfoBV$Grades==-1]=NA
# convert ndar colnames to other ndar colnames
gradesInfoBV$eventname=gradesInfoBV$event_name
gradesInfoBV$subjectkey=gradesInfoBV$src_subject_id
# for tp2, the key is 1 = A's, 2 = B's, 3 = C's, 4 = D's, 5 = F's, -1 = NA
gradesInfoY2=read.delim('~/Downloads/Package_1207225/abcd_saag01.txt')
gradesInfoY2=subset(gradesInfoY2,eventname=='2_year_follow_up_y_arm_1')
gradesInfoY2$sag_grade_type<-as.numeric(gradesInfoY2$sag_grade_type)
# key: 1=100-97,2=96-93,3=92-90,4=89-87,5=86-83,6=82-80,7=79-77,8=76-73,9=72-70,10=69-67,11=66-65,12=0-65,-1=NA,777= no answer
gradesInfoY2$sag_grade_type[gradesInfoY2$sag_grade_type==-1]=NA
gradesInfoY2$sag_grade_type[gradesInfoY2$sag_grade_type==777]=NA
# now convert to be equivalent with timepoint 1 grades measure
ind12=gradesInfoY2$sag_grade_type==12
ind11=gradesInfoY2$sag_grade_type==11
ind10=gradesInfoY2$sag_grade_type==10
ind9=gradesInfoY2$sag_grade_type==9
ind8=gradesInfoY2$sag_grade_type==8
ind7=gradesInfoY2$sag_grade_type==7
ind6=gradesInfoY2$sag_grade_type==6
ind5=gradesInfoY2$sag_grade_type==5
ind4=gradesInfoY2$sag_grade_type==4
ind3=gradesInfoY2$sag_grade_type==3
ind2=gradesInfoY2$sag_grade_type==2
ind1=gradesInfoY2$sag_grade_type==1
#### Set indices to low-res versions
# < 65 becomes failing
gradesInfoY2$sag_grade_type[ind12]=5
# 66-69 = Ds
gradesInfoY2$sag_grade_type[ind11]=4
gradesInfoY2$sag_grade_type[ind10]=4
# 70-79 = Cs
gradesInfoY2$sag_grade_type[ind7]=3
gradesInfoY2$sag_grade_type[ind8]=3
gradesInfoY2$sag_grade_type[ind9]=3
# 80-89 = Bs
gradesInfoY2$sag_grade_type[ind4]=2
gradesInfoY2$sag_grade_type[ind5]=2
gradesInfoY2$sag_grade_type[ind6]=2
# 90+ = As
gradesInfoY2$sag_grade_type[ind1]=1
gradesInfoY2$sag_grade_type[ind2]=1
gradesInfoY2$sag_grade_type[ind3]=1
gradesInfoY2$Grades<-gradesInfoY2$sag_grade_type

###### ∆∆∆∆∆∆∆ create grades info from both of em
NeededColNames=c('subjectkey','eventname','Grades')
gradesInfo<-rbind(gradesInfoBV[,NeededColNames],gradesInfoY2[,NeededColNames])
gradesInfo$Grades<-as.ordered(gradesInfo$Grades)
###### ∆∆∆∆∆∆∆

# merge and count losses
masterdf<-merge(masterdf,gradesInfo,by=c('subjectkey','eventname'))
gradesdim=dim(masterdf)
print(gradesdim)
dif=cbcldim[1]-gradesdim[1]
print(paste0(dif,' rows lost from grades merge, note loss of rows due to no 1 year timepoint'))

### LOAD in ASR data
asr=read.delim('~/Downloads/Package_1207917/pasr01.txt',na.strings=c("","NA"))
masterdf<-merge(masterdf,asr,by=c('subjectkey','eventname','interview_age'))
asrdim=dim(masterdf)
print(asrdim)
dif=gradesdim[1]-asrdim[1]
print(paste0(dif,' rows lost from asr merge'))

# load in a DEAP file for rel_family_ID
DEAP=readRDS('~/Downloads/DEAP-data-download-13.rds')
DEAP$subjectkey<-DEAP$src_subject_id
DEAP$eventname=DEAP$event_name
DEAP=DEAP[,c('rel_family_id','subjectkey','eventname')]
masterdf<-merge(masterdf,DEAP,by=c('subjectkey','eventname'))
deapdim=dim(masterdf)
print(deapdim)
dif=asrdim[1]-deapdim[1]
print(paste0(dif,' rows lost from deap familyID merge'))

### CLEAN data
# subjectkey as factor
masterdf$subjectkey<-as.factor(masterdf$subjectkey)
# convert cbcl scores to numeric
masterdf$cbcl_scr_syn_totprob_r<-as.numeric(masterdf$cbcl_scr_syn_totprob_r)
masterdf$cbcl_scr_syn_internal_r<-as.numeric(masterdf$cbcl_scr_syn_internal_r)
masterdf$cbcl_scr_syn_external_r<-as.numeric(masterdf$cbcl_scr_syn_external_r)
# remove instances of NA tot probs
masterdf=masterdf[!is.na(masterdf$cbcl_scr_syn_totprob_r),]
newDim=dim(masterdf)
print(paste0(newDim[1],' after removing NAs for totprob_r, ',(deapdim[1]- newDim[1]),' lost after removing'))
# and for is empty
masterdf=masterdf[!is.empty(masterdf$cbcl_scr_syn_totprob_r),]
newDim2=dim(masterdf)
print(paste0(newDim2[1],' after removing isempty for totprob_r, ',(newDim[1]- newDim2[1]),' lost after removing'))

```

```{r}
###########∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆##############
###   This chunk processes cognitive data    ###
###########∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆##############

#### LOAD in cognitive data
nihCog=read.delim('~/Downloads/Package_1206930/abcd_tbss01.txt')
othCog=read.delim('~/Downloads/Package_1206930/abcd_ps01.txt')
littleMan=read.delim('~/Downloads/Package_1206931/lmtp201.txt')

# merge in
masterdf<-merge(masterdf,nihCog,by=c('subjectkey','eventname','interview_age'))
newDim3=dim(masterdf)
print(paste0(newDim3[1],' after merging nih toolbox, ',(newDim2[1]- newDim3[1]),' lost after removing'))

masterdf<-merge(masterdf,othCog,by=c('subjectkey','eventname','interview_age'))
newDim4=dim(masterdf)
print(paste0(newDim4[1],' after merging other cognitive measures, ',(newDim3[1]- newDim4[1]),' lost after removing'))

masterdf<-merge(masterdf,littleMan,by=c('subjectkey','eventname','interview_age'))
newDim5=dim(masterdf)
print(paste0(newDim5[1],' after merging little man, ',(newDim4[1]- newDim5[1]),' lost after removing'))

# clean age
masterdf$interview_age<-as.numeric(masterdf$interview_age)
masterdf$interview_age<-as.numeric(masterdf$interview_age)/12
```

```{r}
###########∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆##############
##This chunk preps for cognition factorization##
###########∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆##############

# use thompson 2019 recreation of non nih-tb measures
ind_pea_ravlt = c(which(names(masterdf)=="pea_ravlt_sd_trial_i_tc"),which(names(masterdf)=="pea_ravlt_sd_trial_ii_tc"),
	which(names(masterdf)=="pea_ravlt_sd_trial_iii_tc"),which(names(masterdf)=="pea_ravlt_sd_trial_iv_tc"),
	which(names(masterdf)=="pea_ravlt_sd_trial_v_tc")); names(masterdf)[ind_pea_ravlt];

# set numbers to numeric
masterdf$pea_ravlt_sd_trial_i_tc=as.numeric(masterdf$pea_ravlt_sd_trial_i_tc)
masterdf$pea_ravlt_sd_trial_ii_tc=as.numeric(masterdf$pea_ravlt_sd_trial_ii_tc)
masterdf$pea_ravlt_sd_trial_iii_tc=as.numeric(masterdf$pea_ravlt_sd_trial_iii_tc)
masterdf$pea_ravlt_sd_trial_iv_tc=as.numeric(masterdf$pea_ravlt_sd_trial_vi_tc)
masterdf$pea_ravlt_sd_trial_v_tc=as.numeric(masterdf$pea_ravlt_sd_trial_v_tc)

# total correct across trials
masterdf$pea_ravlt_ld = masterdf$pea_ravlt_sd_trial_i_tc + masterdf$pea_ravlt_sd_trial_ii_tc + masterdf$pea_ravlt_sd_trial_iii_tc + masterdf$pea_ravlt_sd_trial_iv_tc + masterdf$pea_ravlt_sd_trial_v_tc

# change to numeric
masterdf$nihtbx_picvocab_uncorrected<-as.numeric(masterdf$nihtbx_picvocab_uncorrected)
masterdf$nihtbx_flanker_uncorrected<-as.numeric(masterdf$nihtbx_flanker_uncorrected)
masterdf$nihtbx_list_uncorrected<-as.numeric(masterdf$nihtbx_list_uncorrected)
masterdf$nihtbx_cardsort_uncorrected<-as.numeric(masterdf$nihtbx_cardsort_uncorrected)
masterdf$nihtbx_pattern_uncorrected<-as.numeric(masterdf$nihtbx_pattern_uncorrected)
masterdf$nihtbx_picture_uncorrected<-as.numeric(masterdf$nihtbx_picture_uncorrected)
masterdf$nihtbx_reading_uncorrected<-as.numeric(masterdf$nihtbx_reading_uncorrected)
masterdf$pea_wiscv_tss<-as.numeric(masterdf$pea_wiscv_tss)
masterdf$lmt_scr_perc_correct<-as.numeric(masterdf$lmt_scr_perc_correct)

# for isolating PCA dataframe
pcVars=c("nihtbx_picvocab_uncorrected","nihtbx_flanker_uncorrected","nihtbx_pattern_uncorrected","nihtbx_picture_uncorrected","nihtbx_reading_uncorrected","pea_ravlt_ld","lmt_scr_perc_correct")

# test for completeness before running PCA. Better move to calculate ONLY in the sample that we are running analyses on (more technically accurate than PC structure slightly misaligned with sample of interest)
# get other vars of interest to check for complete cases
KidVarsOfInt=c('Grades','cbcl_scr_syn_totprob_r','cbcl_scr_syn_external_r','cbcl_scr_syn_internal_r')
# asr columns of interest to gauge completeness of
ColsOfInt=asr[,c(11:141)]
ASRVarsOfInt=colnames(ColsOfInt)

# only use subjects with both timepoints as complete cases
subjs=unique(masterdf$subjectkey)
for (s in subjs){
  # if there are less than two complete cases of the variables of interest
  if (sum(complete.cases(masterdf[masterdf$subjectkey==s,c(pcVars,KidVarsOfInt,ASRVarsOfInt)]))<2){
    subjs=subjs[subjs!=s]
  }
}
# convert masterdf to df with complete observations for cognition
masterdf=masterdf[masterdf$subjectkey %in% subjs,]

newDim6=dim(masterdf)
print(paste0(newDim6[1],' after retaining only subjs with vars of int at BOTH timepoints, ',(newDim5[1]- newDim6[1]),' lost after removing'))

print(dim(masterdf))

### ∆∆∆
# finish cleaning data for sherlock runs: one family member per family to facilitate random sample
masterdf$id_fam = NULL
# default value of family size (# of children in abcd study)
masterdf$fam_size = 1

# counter index
ind=0

# set each instance of multiple family members to a family ID, as ind
set.seed(1)
for(f in 1:length(unique(masterdf$rel_family_id))){
  # calculate family size
  famsize=sum(masterdf$rel_family_id == unique(masterdf$rel_family_id)[f]) / 2
  masterdf$fam_size[masterdf$rel_family_id == unique(masterdf$rel_family_id)[f]] = famsize
  # note that each  person is represented twice at this point:
  # divide by 2 to take number of visits to number of people, if there's more than 2x visits per family ID, izza family
  # this logic gets hairy. Starting from outside in: > 1 is family size >1, /2 is divided by 2 for two visits, [f] is unique familyID, rel_family_id is place in column of masterdf
  if(famsize>1){
    # remove one from instances where family-id = this relative family id (sequence for siblings, 1:size(Family))
    #print(paste0('family size ',famsize))
    # keep one sib
    kept=sample(seq(1,famsize),1)
    #print(paste0('kept ',kept))
    # use to select one subject id
    famIDs=unique(masterdf$subjectkey[masterdf$rel_family_id == unique(masterdf$rel_family_id)[f]])
    # chosen sib
    keeper=famIDs[kept]
    left=famIDs[-c(kept)]
    # leave rest
    masterdf=masterdf[masterdf$subjectkey!=left,] 
    #print(paste0('left ',left))
    # calc index of family
    ind=ind+1	
    # set index of family
    masterdf$id_fam[masterdf$rel_family_id == unique(masterdf$rel_family_id)[f]] = ind
  }	
}

# make family ID for those with families represented in ABCD
masterdf$rel_family_id=masterdf$id_fam

newDim7=dim(masterdf)
print(paste0(newDim7[1],' after retaining only one subjs per family, ',(newDim6[1]- newDim7[1]),' lost after removing'))

#       NOW 
# THAT'S WHAT I CALL PCAPREP
#       271
# pea_wiscv_tss, nihtbx_list_uncorrected, and nihtbx_cardsort_uncorrected taken out for lack of longitudinal coverage
pcaDf<-masterdf[,pcVars]
```

```{r}
###########∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆##############
###  This chunk runs cognition factorization ###
###########∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆##############

# derive pcs
Y = as.matrix(scale(pcaDf[complete.cases(pcaDf[,pcVars]),pcVars]))
# equiv for binding scores to IDs and eventnames
pcVarsAndIDs=c("nihtbx_picvocab_uncorrected","nihtbx_flanker_uncorrected","nihtbx_pattern_uncorrected","nihtbx_picture_uncorrected","nihtbx_reading_uncorrected","pea_ravlt_ld","lmt_scr_perc_correct","subjectkey","eventname")
Yextended=masterdf[complete.cases(masterdf[,pcVarsAndIDs]),pcVarsAndIDs]
ncomp = 3
y.pca = psych::principal(Y, rotate="varimax", nfactors=ncomp, scores=TRUE)
y.pca$loadings
# assign scores to subjs
Yextended$g<-y.pca$scores[,1]
# merge in cog data
masterdf$g<-Yextended$g
```

```{r}
# save out all timepoints df for bootstrapping both-tp-fits
saveRDS(masterdf,'~/DfWithGrades.rds')
print(dim(masterdf))

# exclude subjs without data for both timepoints
OutDF=masterdf
dimOutDF=dim(OutDF)
OutDFBV=subset(OutDF,eventname=='baseline_year_1_arm_1')
OutDF2Y=subset(OutDF,eventname=='2_year_follow_up_y_arm_1')
# intersection of subjs in both
BothTPsubjs=intersect(OutDFBV$subjectkey,OutDF2Y$subjectkey)
# index out intersection from non tp-split df
OutDF=OutDF[OutDF$subjectkey %in% BothTPsubjs,]
outDf2dim=dim(OutDF)
print(outDf2dim)
dif=dimOutDF[1]-outDf2dim[1]
print(paste0(dif,' rows lost from only using subjs with both timepoints'))
```

```{r}
# make count version of adult P
ASRdfNum<-as.data.frame(lapply(asr[-1,11:141],as.numeric))
ASRtotal=rowSums(ASRdfNum)
# and subtract reverse score items because they were included in sum above, and modeling "happiness" as symmetric to "symptoms" seems like a strong assumption
# reverse scored = face validity AND loading in expected direction
ASRtotal=ASRtotal-ASRdfNum$asr_q02_p
ASRtotal=ASRtotal-ASRdfNum$asr_q04_p
ASRtotal=ASRtotal-ASRdfNum$asr_q15_p
ASRtotal=ASRtotal-ASRdfNum$asr_q73_p
ASRtotal=ASRtotal-ASRdfNum$asr_q80_p
ASRtotal=ASRtotal-ASRdfNum$asr_q88_p
ASRtotal=ASRtotal-ASRdfNum$asr_q106_p
ASRtotal=ASRtotal-ASRdfNum$asr_q109_p
ASRtotal=ASRtotal-ASRdfNum$asr_q123_p

# merge in (first row is colnames)
asr$parentPcount=c(NA,ASRtotal)
# fix asr age for merge
asr$interview_age=as.numeric(asr$interview_age)/12
# set subjectkey to factor for merge
asr$subjectkey<-as.factor(asr$subjectkey)

# merge
OutDF=merge(OutDF,asr,by=c('subjectkey','eventname','interview_age'))
print(dim(OutDF))
saveRDS(OutDF,'~/OutDfFull.rds')

# convert to one row per subj for temporal precedence analyses
OutDFBV=subset(OutDF,eventname=='baseline_year_1_arm_1')
OutDF2Y=subset(OutDF,eventname=='2_year_follow_up_y_arm_1')
OutDFTmpPrec<-merge(OutDFBV,OutDF2Y,by='subjectkey')
print(dim(OutDFTmpPrec))

saveRDS(OutDFTmpPrec,'~/OutDFTmpPrec.rds')
```


```{r}
###########∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆##############
## Now we are at sample construction for ES   ##
###########∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆∆##############

### LOAD in residential deprivation. Warning that this .txt file is gnarly. IMO should cross reference sex subjectkey and event name to confirm that non missing data is feasibly accurate
resIn=read.delim('~/Downloads/Package_1209596/abcd_rhds01.txt',sep="\t")
resInColNames=resIn[1,]
# will print warning on end-of-file being within quotes: textedit reports 117,151,883 " within the file, suggesting there is at least one missing or extraneous quotation mark somewhere 
StartingResDim=dim(resIn)
print(paste0('Starting Residential df dim: ',StartingResDim[1]))
# remove 1 and 3_year follow up as other variables aren't populated there
resIn=resIn[resIn$eventname!='1_year_follow_up_y_arm_1',]
resIn=resIn[resIn$eventname!='3_year_follow_up_y_arm_1',]
# remove colname row
resIn=resIn[-1,]
JustBV2y=dim(resIn)
print(paste0('After removing 1 year and 3 year: ',JustBV2y[1]))
# there are some missing subjectkeys, and some that are nonsensical numbers (i.e., 722.56, -0.306). Remove em both unless someone can justify them
resIn=resIn[!is.empty(resIn$subjectkey),]
print(dim(resIn))
# remove those that don't start with feasible ID
FeasibleIDs=grep('NDAR_',resIn$subjectkey)
resIn=resIn[FeasibleIDs,]
# remove those that are a million characters (likely a line read error)
Selected <- sapply(seq_len(nrow(resIn)),
                      function(i) nchar(resIn$subjectkey[i]))
print(max(Selected))
# looks like there's one $#!@ing row with a 61,304 chareacter length subject ID mussing this up. Should be 16
resIn=resIn[Selected<17,]
ValidSubjIDDim=dim(resIn)
print(paste0('After removing overtly invalid subjIDs: ',ValidSubjIDDim[1]))
# there are more subject IDs than expected given number of unique subject IDs. Go ahead and split by timepoint finna slueth
resInBV=resIn[resIn$eventname!='baseline_year_1_arm_1',]
resIn2Y=resIn[resIn$eventname!='2_year_follow_up_y_arm_1',]
#BV
# remove subjects who are repeated within-timepoint
bvSubjs=table(as.factor(resInBV$subjectkey))
## report subj ids used more than once and times used
bvSubjsdf=data.frame(bvSubjs[bvSubjs<2])
UniqueSubjs=bvSubjsdf$Var1
# throw out repeated subjs unless someone can justify them
resInBV=resInBV[resInBV$subjectkey %in% UniqueSubjs,]
#2Year
# remove subjects who are repeated within-timepoint
Y2Subjs=table(as.factor(resIn2Y$subjectkey))
## report subj ids used more than once and times used
Y2Subjsdf=data.frame(Y2Subjs[Y2Subjs<2])
UniqueSubjs=Y2Subjsdf$Var1
# throw out repeated subjs unless someone can justify them
resIn2Y=resIn2Y[resIn2Y$subjectkey %in% UniqueSubjs,]
# reunited and it feeeels so good
resIn=rbind(resInBV,resIn2Y)
print(paste0('After removing after removing further monkey-business in subjIDs: ',dim(resIn)[1]))

# looks like address one is the only with <94% empty values. Let's only use that
addy1=grep('reshist_addr1',colnames(resIn))
resIn=resIn[,c(1:9,addy1)]
# evaluate data availability
colMeans(is.empty(resIn))
# empty out if reshist deriv score is goen
resIn=resIn[resIn$reshist_addr1_adi_wsum!='',]

# OK, now only subjects with both timepoints
subjs=unique(resIn$subjectkey)
for (s in subjs){
  # if there are more than one complete cases of the variables of interest
  if (sum(complete.cases(resIn$reshist_addr1_adi_wsum[resIn$subjectkey==s]))>1){
    subjs=subjs[subjs!=s]
  }
}

# get dimensions and compare
dimOutDF=dim(OutDF)
OutDFRes=merge(OutDF,resIn,by=c('subjectkey','eventname'))
dimOutDF2=dim(OutDFRes)
dif=dimOutDF[1]-dimOutDF2[1]
print(paste0('After removing after retaining only subjs with 2 obs. : ',dimOutDF2[1]))
print(paste0(dif, ' lost'))
```

```{r}
# ensure site coverage
### LOAD in ParticipantsTSV for parent income and edu background
# ordained sample split
participantsTSV=read.delim('~/Downloads/participants.tsv',sep="\t")
participantsTSV$subjectkey<-participantsTSV$participant_id
# reformat participant IDs so they match everything else
participantsTSV$subjectkey<-gsub('sub-','',participantsTSV$subjectkey)
participantsTSV$subjectkey<-as.factor(gsub('NDARINV','NDAR_INV',participantsTSV$subjectkey))
participantsTSV$eventname=participantsTSV$session_id
### ∆∆∆ Apparently odd issue where subjects are repeated. Maybe rows are identical?
# query. get table of subject ids
b=table(as.factor(participantsTSV$participant_id))
# report subj ids used more than once and times used
bdf=data.frame(b[b>1])
SubjsRepeated=bdf$Var1
# well some rows with the same participant IDs have different sites, can't use that unless it makes sense\
dimB4repRem=dim(participantsTSV)
# remove repeated subjs
participantsTSV=participantsTSV[!participantsTSV$participant_id %in% SubjsRepeated,]
dif=dimB4repRem[1]-dim(participantsTSV)[1]
print(paste0(dif/2,' Participants lost from ambiguously repeated pt ids in participants.tsv'))

# well some rows with the same participant IDs have different sites, can't use that unless it makes sense\
dimB4repRem=dim(participantsTSV)
# remove repeated subjs
participantsTSV=participantsTSV[!participantsTSV$participant_id %in% SubjsRepeated,]

### merge in for fam income and parent edu
OutDF=merge(OutDFRes,participantsTSV,by=c('subjectkey'))
# take out na incomes
OutDF=OutDF[OutDF$income!=777,]
OutDF=OutDF[OutDF$income!=999,]
# plot baseline only for better sense of site coverage
plotdf<-na.omit(data.frame(OutDF$reshist_addr1_adi_wsum[OutDF$eventname.x=='baseline_year_1_arm_1'],OutDF$site[OutDF$eventname.x=='baseline_year_1_arm_1']))
colnames(plotdf)=c('ResDep','Site')
plotdf$Site<-as.factor(plotdf$Site)
plotdf$ResDep<-as.numeric(plotdf$ResDep)
ggplot(aes(x=ResDep,y=Site),data=plotdf)+geom_boxplot()+xlab('Area Deprivation Index')+ylab('Site')

# race
OutDF=OutDF[OutDF$race_ethnicity!=888,]
OutDF$race_ethnicity<-as.factor(OutDF$race_ethnicity)

# parental edu
OutDF=OutDF[OutDF$parental_education!=888,]
OutDF=OutDF[OutDF$parental_education!=777,]
OutDF$parental_education<-as.ordered(OutDF$parental_education)
dim(OutDF)

#### load in youth life events
yle=read.delim('~/Downloads/Package_1209596/abcd_yle01.txt')
#### load in parental life events
ple=read.delim('~/Downloads/Package_1209596/abcd_ple01.txt')
#### Some will need to be restropective 

```

```{r}
### prelim models
library(mgcv)

OutDF$reshist_addr1_adi_wsum<-as.numeric(OutDF$reshist_addr1_adi_wsum)

model_p=gam(cbcl_scr_syn_totprob_r~race_ethnicity+s(g)+Grades+s(parentPcount)+s(reshist_addr1_adi_wsum)+s(income)+parental_education,OutDF,family=nb())
model_int=gam(cbcl_scr_syn_internal_r~race_ethnicity+s(g)+Grades+s(parentPcount)+s(reshist_addr1_adi_wsum)+s(income)+parental_education,OutDF,family=nb())
model_ext=gam(cbcl_scr_syn_external_r~race_ethnicity+s(g)+Grades+s(parentPcount)+s(reshist_addr1_adi_wsum)+s(income)+parental_education,OutDF,family=nb())



# prelim temporal precedence models
OutDFBV2=subset(OutDF,eventname.x=='baseline_year_1_arm_1')
OutDF2Y2=subset(OutDF,eventname.x=='2_year_follow_up_y_arm_1')
OutDFTmpPrec2<-merge(OutDFBV2,OutDF2Y2,by='subjectkey')

# lost major # of subjects from residential data
print(dim(OutDFTmpPrec2))

model_p=gam(cbcl_scr_syn_totprob_r.y~s(cbcl_scr_syn_totprob_r.y)+race_ethnicity.x+(g.x)+Grades.x+s(parentPcount.x)+s(reshist_addr1_adi_wsum.x)+s(income.x)+parental_education.x,OutDFTmpPrec2,family=nb())
model_int=gam(cbcl_scr_syn_internal_r.y~s(cbcl_scr_syn_internal_r.x)+race_ethnicity.x+s(g.x)+Grades.x+s(parentPcount.x)+s(reshist_addr1_adi_wsum.x)+s(income.x)+parental_education.x,OutDFTmpPrec2,family=nb())
model_ext=gam(cbcl_scr_syn_external_r.y~s(cbcl_scr_syn_external_r.x)+race_ethnicity+s(g.x)+Grades.x+s(parentPcount.x)+s(reshist_addr1_adi_wsum.x)+s(income.x)+parental_education.x,OutDFTmpPrec2,family=nb())
```