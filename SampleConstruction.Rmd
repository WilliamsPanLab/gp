---
title: "SampleConstruction"
author: "Adam"
output: github_document
date: "2023-01-25"
---

```{r}
# libraries
library(rapportools)

# load in data
cbcl=read.delim('~/Downloads/Package_1205735/abcd_cbcl01.txt')
cbcls=read.delim('/Users/panlab/Downloads/Package_1205735/abcd_cbcls01.txt')
# subset timepoints
cbclsBV=subset(cbcls,eventname=='baseline_year_1_arm_1')
cbcls2=subset(cbcls,eventname=='2_year_follow_up_y_arm_1')
#### add non summary items to cbcl for schoolwork q
# subset timepoints
cbclBV=subset(cbcl,eventname=='baseline_year_1_arm_1')
cbcl2=subset(cbcl,eventname=='2_year_follow_up_y_arm_1')
# merge with other cbcl
cbclsBV=merge(cbclsBV,cbclBV,by=c('subjectkey','eventname'))
cbcls2=merge(cbcls2,cbcl2,by=c('subjectkey','eventname'))

# load in ASR
asr=read.delim('~/Downloads/Package_1207917/pasr01.txt',na.strings=c("","NA"))

# load in clinicalish data
clinc1=read.delim('/Users/panlab/Downloads/Package_1205908/abcd_ksad501.txt')
kBV=subset(clinc1,eventname=='baseline_year_1_arm_1')
k1=subset(clinc1,eventname=='1_year_follow_up_y_arm_1')
k2=subset(clinc1,eventname=='2_year_follow_up_y_arm_1')
clinc1_p=read.delim('/Users/panlab/Downloads/Package_1205908/abcd_ksad01.txt')

# read in grades, annoying distinction between tp1 measure (decent granulrity) and tp2 measure (high granularity)
gradesInfoBV=readRDS('~/Downloads/DEAP-data-download-13.rds')
gradesInfoBV=subset(gradesInfoBV,event_name=='baseline_year_1_arm_1')
gradesInfoBV$Grades<-as.numeric(gradesInfoBV$ksads_back_grades_in_school_p)
gradesInfoBV$Grades[gradesInfoBV$Grades==-1]=NA
gradesInfoBV$eventname=gradesInfoBV$event_name
gradesInfoBV$subjectkey=gradesInfoBV$src_subject_id
# for this one, the key is 1 = A's, 2 = B's, 3 = C's, 4 = D's, 5 = F's, -1 = NA
gradesInfoY2=read.delim('~/Downloads/Package_1207225/abcd_saag01.txt')
gradesInfoY2=subset(gradesInfoY2,eventname=='2_year_follow_up_y_arm_1')
gradesInfoY2$sag_grade_type<-as.numeric(gradesInfoY2$sag_grade_type)
# key: 1=100-97,2=96-93,3=92-90,4=89-87,5=86-83,6=82-80,7=79-77,8=76-73,9=72-70,10=69-67,11=66-65,12=0-65,-1=NA,777= no answer
gradesInfoY2$sag_grade_type[gradesInfoY2$sag_grade_type==-1]=NA
gradesInfoY2$sag_grade_type[gradesInfoY2$sag_grade_type==777]=NA
# now convert to be equivalent with timepoint 1 grades measure
ind12=gradesInfoY2$sag_grade_type==12
ind11=gradesInfoY2$sag_grade_type==11
ind10=gradesInfoY2$sag_grade_type==10
ind9=gradesInfoY2$sag_grade_type==9
ind8=gradesInfoY2$sag_grade_type==8
ind7=gradesInfoY2$sag_grade_type==7
ind6=gradesInfoY2$sag_grade_type==6
ind5=gradesInfoY2$sag_grade_type==5
ind4=gradesInfoY2$sag_grade_type==4
ind3=gradesInfoY2$sag_grade_type==3
ind2=gradesInfoY2$sag_grade_type==2
ind1=gradesInfoY2$sag_grade_type==1
#### Set indices to low-res versions
# < 65 becomes failing
gradesInfoY2$sag_grade_type[ind12]=5
# 66-69 = Ds
gradesInfoY2$sag_grade_type[ind11]=4
gradesInfoY2$sag_grade_type[ind10]=4
# 70-79 = Cs
gradesInfoY2$sag_grade_type[ind7]=3
gradesInfoY2$sag_grade_type[ind8]=3
gradesInfoY2$sag_grade_type[ind9]=3
# 80-89 = Bs
gradesInfoY2$sag_grade_type[ind4]=2
gradesInfoY2$sag_grade_type[ind5]=2
gradesInfoY2$sag_grade_type[ind6]=2
# 90+ = As
gradesInfoY2$sag_grade_type[ind1]=1
gradesInfoY2$sag_grade_type[ind2]=1
gradesInfoY2$sag_grade_type[ind3]=1
gradesInfoY2$Grades<-gradesInfoY2$sag_grade_type
###### ∆∆∆∆∆∆∆ create grades info from both of em
NeededColNames=c('subjectkey','eventname','Grades')
gradesInfo<-rbind(gradesInfoBV[,NeededColNames],gradesInfoY2[,NeededColNames])
gradesInfo$Grades<-as.ordered(gradesInfo$Grades)

################################### ∆∆∆∆∆

# initialize master df
masterdf<-merge(cbcls,cbcl,by=c('subjectkey','eventname','interview_age','src_subject_id'))
masterdf<-merge(masterdf,clinc1,by=c('subjectkey','eventname','interview_age','src_subject_id'))
masterdf<-merge(masterdf,clinc1_p,by=c('subjectkey','eventname','interview_age','src_subject_id'))
masterdf<-merge(masterdf,gradesInfo,by=c('subjectkey','eventname'))

# load in a DEAP file for rel_family_ID
DEAP=readRDS('~/Downloads/DEAP-data-download-13.rds')
DEAP$subjectkey<-DEAP$src_subject_id
DEAP$eventname=DEAP$event_name
DEAP=DEAP[,c('rel_family_id','subjectkey','eventname')]

#### clean data

# convert to numeric
masterdf$interview_age<-as.numeric(masterdf$interview_age)
# cbcl sum indep. of the q of interest
masterdf$cbcl_scr_syn_totprob_r<-as.numeric(masterdf$cbcl_scr_syn_totprob_r)-as.numeric(masterdf$cbcl_q61_p)
masterdf$cbcl_q61_p<-as.ordered(masterdf$cbcl_q61_p)
masterdf$subjectkey<-as.factor(masterdf$subjectkey)
# clean data
masterdf$cbcl_scr_syn_totprob_r<-as.numeric(masterdf$cbcl_scr_syn_totprob_r)
masterdf$cbcl_scr_syn_internal_r<-as.numeric(masterdf$cbcl_scr_syn_internal_r)
masterdf$cbcl_scr_syn_external_r<-as.numeric(masterdf$cbcl_scr_syn_external_r)
masterdf$subjectkey<-as.factor(masterdf$subjectkey)
masterdf$cbcl_q61_p<-as.ordered(masterdf$cbcl_q61_p)
# remove instances of NA tot probs
masterdf=masterdf[!is.na(masterdf$cbcl_scr_syn_totprob_r),]
# and for q61
masterdf=masterdf[!is.na(masterdf$cbcl_q61_p),]
# and for is empty
masterdf=masterdf[!is.empty(masterdf$cbcl_scr_syn_totprob_r),]
masterdf=masterdf[!is.empty(masterdf$cbcl_q61_p),]
# r can't take the hint
masterdf=masterdf[!masterdf$cbcl_scr_syn_totprob_r=='',]
masterdf=masterdf[!masterdf$cbcl_q61_p=='',]

#### now load in cognitive data
nihCog=read.delim('~/Downloads/Package_1206930/abcd_tbss01.txt')
othCog=read.delim('~/Downloads/Package_1206930/abcd_ps01.txt')
littleMan=read.delim('~/Downloads/Package_1206931/lmtp201.txt')

# merge in
masterdf<-merge(masterdf,nihCog,by=c('subjectkey','eventname','interview_age'))
masterdf<-merge(masterdf,othCog,by=c('subjectkey','eventname','interview_age'))
masterdf<-merge(masterdf,littleMan,by=c('subjectkey','eventname','interview_age'))
masterdf<-merge(masterdf,DEAP,by=c('subjectkey','eventname'))
# clean age
masterdf$interview_age<-as.numeric(masterdf$interview_age)/12
```

```{r}
# use thompson 2019 recreation of non nih-tb measures
ind_pea_ravlt = c(which(names(masterdf)=="pea_ravlt_sd_trial_i_tc"),which(names(masterdf)=="pea_ravlt_sd_trial_ii_tc"),
	which(names(masterdf)=="pea_ravlt_sd_trial_iii_tc"),which(names(masterdf)=="pea_ravlt_sd_trial_iv_tc"),
	which(names(masterdf)=="pea_ravlt_sd_trial_v_tc")); names(masterdf)[ind_pea_ravlt];

# set numbers to numeric
masterdf$pea_ravlt_sd_trial_i_tc=as.numeric(masterdf$pea_ravlt_sd_trial_i_tc)
masterdf$pea_ravlt_sd_trial_ii_tc=as.numeric(masterdf$pea_ravlt_sd_trial_ii_tc)
masterdf$pea_ravlt_sd_trial_iii_tc=as.numeric(masterdf$pea_ravlt_sd_trial_iii_tc)
masterdf$pea_ravlt_sd_trial_iv_tc=as.numeric(masterdf$pea_ravlt_sd_trial_vi_tc)
masterdf$pea_ravlt_sd_trial_v_tc=as.numeric(masterdf$pea_ravlt_sd_trial_v_tc)

# total correct across trials
masterdf$pea_ravlt_ld = masterdf$pea_ravlt_sd_trial_i_tc + masterdf$pea_ravlt_sd_trial_ii_tc + masterdf$pea_ravlt_sd_trial_iii_tc + masterdf$pea_ravlt_sd_trial_iv_tc + masterdf$pea_ravlt_sd_trial_v_tc

```

```{r}
#### calculate PCs

# change to numeric
masterdf$nihtbx_picvocab_uncorrected<-as.numeric(masterdf$nihtbx_picvocab_uncorrected)
masterdf$nihtbx_flanker_uncorrected<-as.numeric(masterdf$nihtbx_flanker_uncorrected)
masterdf$nihtbx_list_uncorrected<-as.numeric(masterdf$nihtbx_list_uncorrected)
masterdf$nihtbx_cardsort_uncorrected<-as.numeric(masterdf$nihtbx_cardsort_uncorrected)
masterdf$nihtbx_pattern_uncorrected<-as.numeric(masterdf$nihtbx_pattern_uncorrected)
masterdf$nihtbx_picture_uncorrected<-as.numeric(masterdf$nihtbx_picture_uncorrected)
masterdf$nihtbx_reading_uncorrected<-as.numeric(masterdf$nihtbx_reading_uncorrected)
masterdf$pea_wiscv_tss<-as.numeric(masterdf$pea_wiscv_tss)
masterdf$lmt_scr_perc_correct<-as.numeric(masterdf$lmt_scr_perc_correct)

# for isolating PCA df
pcVars=c("nihtbx_picvocab_uncorrected","nihtbx_flanker_uncorrected","nihtbx_pattern_uncorrected","nihtbx_picture_uncorrected","nihtbx_reading_uncorrected","pea_ravlt_ld","lmt_scr_perc_correct")

# get other vars of interest to check for complete cases
KidVarsOfInt=c('Grades','cbcl_scr_syn_totprob_r','cbcl_scr_syn_external_r','cbcl_scr_syn_internal_r')


# columns of interest to gauge completeness of
ColsOfInt=asr[,c(11:141)]
ASRVarsOfInt=colnames(ColsOfInt)
# clean age in ASR
asr$interview_age<-as.numeric(asr$interview_age)/12
# merge with master
masterdf=merge(asr,masterdf,by=c('subjectkey','eventname','interview_age'))

```

```{r}
# only use subjects with both timepoints as complete cases
subjs=unique(masterdf$subjectkey)
for (s in subjs){
  # if there are less than two complete cases of the variables of interest
  if (sum(complete.cases(masterdf[masterdf$subjectkey==s,c(pcVars,KidVarsOfInt,ASRVarsOfInt)]))<2){
    subjs=subjs[subjs!=s]
  }
}
# convert masterdf to df with complete observations for cognition
masterdf=masterdf[masterdf$subjectkey %in% subjs,]

print(dim(masterdf))
```




```{r}
# finish cleaning data for sherlock runs: one family member per family to facilitate random sample
masterdf$id_fam = NULL
# default value of family size (# of children in abcd study)
masterdf$fam_size = 1

# counter index
ind=0

# set each instance of multiple family members to a family ID, as ind
set.seed(1)
for(f in 1:length(unique(masterdf$rel_family_id))){
  # calculate family size
  famsize=sum(masterdf$rel_family_id == unique(masterdf$rel_family_id)[f]) / 2
  masterdf$fam_size[masterdf$rel_family_id == unique(masterdf$rel_family_id)[f]] = famsize
  # note that each  person is represented twice at this point:
  # divide by 2 to take number of visits to number of people, if there's more than 2x visits per family ID, izza family
  # this logic gets hairy. Starting from outside in: > 1 is family size >1, /2 is divided by 2 for two visits, [f] is unique familyID, rel_family_id is place in column of masterdf
  if(famsize>1){
    # remove one from instances where family-id = this relative family id (sequence for siblings, 1:size(Family))
    #print(paste0('family size ',famsize))
    # keep one sib
    kept=sample(seq(1,famsize),1)
    #print(paste0('kept ',kept))
    # use to select one subject id
    famIDs=unique(masterdf$subjectkey[masterdf$rel_family_id == unique(masterdf$rel_family_id)[f]])
    # chosen sib
    keeper=famIDs[kept]
    left=famIDs[-c(kept)]
    # leave rest
    masterdf=masterdf[masterdf$subjectkey!=left,] 
    #print(paste0('left ',left))
    # calc index of family
    ind=ind+1	
    # set index of family
    masterdf$id_fam[masterdf$rel_family_id == unique(masterdf$rel_family_id)[f]] = ind
  }	
}

# make family ID for those with families represented in ABCD
masterdf$rel_family_id=masterdf$id_fam

print(dim(masterdf))

# pea_wiscv_tss, nihtbx_list_uncorrected, and nihtbx_cardsort_uncorrected taken out for lack of longitudinal coverage
pcaDf<-masterdf[,pcVars]
```

```{r}
# derive Cognitive PCS

# derive pcs
Y = as.matrix(scale(pcaDf[complete.cases(pcaDf[,pcVars]),pcVars]))
# equiv for binding scores to IDs and eventnames
pcVarsAndIDs=c("nihtbx_picvocab_uncorrected","nihtbx_flanker_uncorrected","nihtbx_pattern_uncorrected","nihtbx_picture_uncorrected","nihtbx_reading_uncorrected","pea_ravlt_ld","lmt_scr_perc_correct","subjectkey","eventname")
Yextended=masterdf[complete.cases(masterdf[,pcVarsAndIDs]),pcVarsAndIDs]
ncomp = 3
y.pca = psych::principal(Y, rotate="varimax", nfactors=ncomp, scores=TRUE)
y.pca$loadings
# assign scores to subjs
Yextended$g<-y.pca$scores[,1]
# merge in cog data

masterdf$g<-Yextended$g
```

```{r}
# save out all timepoints df for bootstrapping both-tp-fits
saveRDS(masterdf,'~/DfWithGrades.rds')
print(dim(masterdf))
#### ∆∆∆ Note this is without adult P
```

```{r}
# derive adult p

# columns of interest to gauge completeness of
ColsOfInt=asr[,c(11:141)]
# retain complete cases
completeInd=ColsOfInt[rowSums(is.na(ColsOfInt)) == 0,]
# merge with master
masterdf=merge(asr,masterdf,by=c('subjectkey','eventname','interview_age'))
ASRcolnames=asr[1,]

#######MICHELI APPROACH TO FACTOR DELINEATION OF ASR
# composite creation

# polycors to eval if tp1 vars still meet criterion
library(polycor)
# subset asr into timepoints
asrBV=subset(asr,eventname=='baseline_year_1_arm_1')
asr2=subset(asr,eventname=='2_year_follow_up_y_arm_1')

# save subjIDs
asrBVSubjs=asrBV$subjectkey
asr2Subjs=asr2$subjectkey
# isolate asr qs
asrBV_qs=asrBV[,11:141]
asr2_qs=asr2[,11:141]
# fix character specification for variables
asrBV_qs <- as.data.frame(lapply(asrBV_qs, as.ordered))
asr2_qs <- as.data.frame(lapply(asr2_qs, as.ordered))
# polycormat
asrBV_qs_cormat=hetcor(asrBV_qs)
asr2_qs_cormat=hetcor(asr2_qs)
# ok piecemeal go through and find rows with >.75 cors
for (i in 1:dim(asrBV_qs_cormat$correlations)[1]){
  # a single 1 is expected for diagonal
  if (sum(asrBV_qs_cormat$correlations[i,]>.75)>1){
    # if it is > .75 in both tps
    if (sum(asr2_qs_cormat$correlations[i,]>.75)>1){
      print('Correlated Items:')
      # + 10 because 11th col is first col
      BVoverCorrelateds<-colnames(ASRcolnames[1,which(asrBV_qs_cormat$correlations[i,]>.75)+10])
      year2overCorrelateds<-colnames(ASRcolnames[1,which(asr2_qs_cormat$correlations[i,]>.75)+10])
      intersection=intersect(BVoverCorrelateds,year2overCorrelateds)
      print(unlist(ASRcolnames[intersection]))
      print('-------')
      }
    }
}
```

```{r}
#### merge timepoints
mergedTPs=rbind(asrBV_qs,asr2_qs)
# retain subjsIDs and 
mergedTPsSubjs<-c(asrBVSubjs,asr2Subjs)
mergedTPsEventName<-rep()
mergedTPs <- as.data.frame(lapply(mergedTPs, as.numeric))

# destroyer composite
mergedTPs$asr_destroyer=round((mergedTPs$asr_q20_p+mergedTPs$asr_q21_p)/2)
# remove constituent variables
mergedTPs=subset(mergedTPs, select = -c(`asr_q20_p`,`asr_q21_p`))

# hallucinations composite
mergedTPs$asr_halluc=round((mergedTPs$asr_q40_p+mergedTPs$asr_q70_p)/2)
# remove constituent variables
mergedTPs=subset(mergedTPs, select = -c(`asr_q40_p`,`asr_q70_p`))

# Odd composite
mergedTPs$asr_odd=round((mergedTPs$asr_q84_p+mergedTPs$asr_q85_p)/2)
# remove constituent variables
mergedTPs=subset(mergedTPs, select = -c(`asr_q84_p`,`asr_q85_p`))

# sick composite
mergedTPs$asr_sick=round((mergedTPs$asr_q56c_p+mergedTPs$asr_q56g_p)/2)
# remove constituent variables
mergedTPs=subset(mergedTPs, select = -c(`asr_q56c_p`,`asr_q56g_p`))

# clumsy composite
mergedTPs$asr_clumsy=round((mergedTPs$asr_q36_p+mergedTPs$asr_q62_p)/2)
# remove constituent variables
mergedTPs=subset(mergedTPs, select = -c(`asr_q36_p`,`asr_q62_p`))

# anxious composite
mergedTPs$asr_anx=round((mergedTPs$asr_q45_p+mergedTPs$asr_q50_p)/2)
# remove constituent variables
mergedTPs=subset(mergedTPs, select = -c(`asr_q45_p`,`asr_q50_p`))

# swinger composite
mergedTPs$asr_swing=round((mergedTPs$asr_q55_p+mergedTPs$asr_q87_p)/2)
# remove constituent variables
mergedTPs=subset(mergedTPs, select = -c(`asr_q55_p`,`asr_q87_p`))

# insolvent composite
mergedTPs$asr_insolvent=round((mergedTPs$asr_q114_p+mergedTPs$asr_q117_p)/2)
# remove constits
mergedTPs=subset(mergedTPs, select = -c(`asr_q114_p`,`asr_q117_p`))
#####################

## run pca on asr
pcaDf_p<-mergedTPs
pcaDf_p$SubjsNames<-mergedTPsSubjs
pcaDf_p$eventname<-c(rep('baseline_year_1_arm_1',dim(asrBV)[1]),rep('2_year_follow_up_y_arm_1',dim(asr2)[1]))
# remove NA vars
pcaDf_p_Complete<-pcaDf_p[complete.cases(pcaDf_p),]
pcaDf_p_CompleteSubjs<-pcaDf_p_Complete$SubjsNames
pcaDf_p_CompleteEventNames<-pcaDf_p_Complete$eventname
pcaDf_p=subset(pcaDf_p, select = -c(`eventname`,`SubjsNames`))
# isolate numeric
pcaDf_p_num<-pcaDf_p_Complete[,1:123]
# convert to numeric for pca
pcaDf_p_num <- as.data.frame(lapply(pcaDf_p_num, as.numeric))
# derive pcs
pcaMat_p_complete = as.matrix(scale(pcaDf_p_num))
ncomp = 1
y.pca = psych::principal(pcaMat_p_complete, rotate="geomin", nfactors=ncomp, scores=TRUE)
y.pca$loadings

subjPvalues=data.frame(pcaDf_p_CompleteSubjs,pcaDf_p_CompleteEventNames,y.pca$scores[,1])
colnames(subjPvalues)<-c('subjectkey','eventname','parentP')

OutDF=merge(masterdf,subjPvalues,by=c('subjectkey','eventname'))
```

```{r}
# make count version
ASRdfNum<-as.data.frame(lapply(asr[-1,11:141],as.numeric))
ASRtotal=rowSums(ASRdfNum)
# and subtract reverse score items because they were included in sum above, and modeling "happiness" as symmetric to "symptoms" seems like a strong assumption
# reverse scored = face validity AND loading in expected direction
ASRtotal=ASRtotal-ASRdfNum$asr_q02_p
ASRtotal=ASRtotal-ASRdfNum$asr_q04_p
ASRtotal=ASRtotal-ASRdfNum$asr_q15_p
ASRtotal=ASRtotal-ASRdfNum$asr_q73_p
ASRtotal=ASRtotal-ASRdfNum$asr_q80_p
ASRtotal=ASRtotal-ASRdfNum$asr_q88_p
ASRtotal=ASRtotal-ASRdfNum$asr_q106_p
ASRtotal=ASRtotal-ASRdfNum$asr_q109_p
ASRtotal=ASRtotal-ASRdfNum$asr_q123_p

# merge in (first row is colnames)
asr$parentPcount=c(NA,ASRtotal)
OutDF=merge(OutDF,asr,by=c('subjectkey','eventname','interview_age'))
print(dim(OutDF))
saveRDS(OutDF,'~/OutDfFull.rds')

# convert to one row per subj for temporal precedence analyses
OutDFBV=subset(OutDF,eventname=='baseline_year_1_arm_1')
OutDF2Y=subset(OutDF,eventname=='2_year_follow_up_y_arm_1')
OutDFTmpPrec<-merge(OutDFBV,OutDF2Y,by='subjectkey')
print(dim(OutDFTmpPrec))

saveRDS(OutDFTmpPrec,'~/OutDFTmpPrec.rds')
```